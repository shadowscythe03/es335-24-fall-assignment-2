{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "    \n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x,theta_0,theta_1):\n",
    "    return theta_1*x + theta_0\n",
    "def mse(y_hat,y):\n",
    "    return torch.mean((y_hat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n"
     ]
    }
   ],
   "source": [
    "x1_torch = torch.from_numpy(x1)\n",
    "y_torch = torch.from_numpy(y)\n",
    "theta_0 = torch.tensor(1.0,requires_grad=True)\n",
    "theta_1 = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "y_hat = reg(x1_torch,theta_0,theta_1)\n",
    "loss = mse(y_hat,y_torch)\n",
    "loss.backward()\n",
    "theta_0_true_grad = theta_0.grad.item()\n",
    "theta_1_true_grad = theta_1.grad.item()\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n",
      "The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset: -5.447053959220648 -1.194682613387704\n"
     ]
    }
   ],
   "source": [
    "t0_stochastic_gradients = []\n",
    "t1_stochastic_gradients = []\n",
    "for i in range(num_samples):\n",
    "    theta_0.grad = None\n",
    "    theta_1.grad = None\n",
    "    y_hat = reg(x1_torch[i],theta_0,theta_1)\n",
    "    loss = mse(y_hat,y_torch[i])\n",
    "    loss.backward()\n",
    "    t0_stochastic_gradients.append(theta_0.grad.item())\n",
    "    t1_stochastic_gradients.append(theta_1.grad.item())\n",
    "\n",
    "t0_avg_grad = sum(t0_stochastic_gradients)/num_samples\n",
    "t1_avg_grad = sum(t1_stochastic_gradients)/num_samples\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)\n",
    "print('The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset:',t0_avg_grad,t1_avg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between avg of stochastic and true grad is 4.9918890354661016e-08 1.527369031251169e-08\n"
     ]
    }
   ],
   "source": [
    "print('The difference between avg of stochastic and true grad is',abs(theta_0_true_grad-t0_avg_grad),abs(theta_1_true_grad-t1_avg_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_batch_grad(x,y,lr = 0.01,epsilon = 0.001,epochs = 5000):\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    epoch = 1\n",
    "    loss = torch.tensor(float('inf'))\n",
    "    if epochs is not None:\n",
    "        while loss.item()>=epsilon:\n",
    "            y_hat = reg(x,theta_0,theta_1)\n",
    "            loss = mse(y_hat,y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                theta_0.data = theta_0-lr*theta_0.grad\n",
    "                theta_1.data = theta_1-lr*theta_1.grad\n",
    "\n",
    "            t0_hist.append(theta_0.item())\n",
    "            t1_hist.append(theta_1.item())\n",
    "            # print(f'Epoch:{epoch}--> Loss:{loss.item()}')\n",
    "            theta_0.grad.zero_()\n",
    "            theta_1.grad.zero_()\n",
    "            epoch += 1\n",
    "            if epoch>=epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    else:\n",
    "        while loss.item()>=epsilon:\n",
    "            y_hat = reg(x,theta_0,theta_1)\n",
    "            loss = mse(y_hat,y)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                theta_0.data = theta_0-lr*theta_0.grad\n",
    "                theta_1.data = theta_1-lr*theta_1.grad\n",
    "\n",
    "            t0_hist.append(theta_0.item())\n",
    "            t1_hist.append(theta_1.item())\n",
    "            # print(f'Epoch:{epoch}--> Loss:{loss.item()}')\n",
    "            theta_0.grad.zero_()\n",
    "            theta_1.grad.zero_()\n",
    "            epoch += 1\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "                \n",
    "    print(f'Full Batch Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses, theta_0.grad.item(), theta_1.grad.item(),t0_hist,t1_hist\n",
    "def mini_batch_grad(x,y,lr = 0.01,batch_size = 2,epsilon = 0.001, epochs = 1500):\n",
    "    num_samples = x.size(0)\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    avg_epoch_loss = float('inf')\n",
    "    num_samples_per_batch = num_samples//batch_size\n",
    "    epoch = 1\n",
    "    if epochs is not None:\n",
    "        while avg_epoch_loss >= epsilon:\n",
    "            batch = torch.randperm(num_samples)\n",
    "            epoch_loss = 0\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "                indice = batch[i:i+batch_size]\n",
    "                mini_x,mini_y = x[indice],y[indice]\n",
    "                mini_y_hat = reg(mini_x, theta_0,theta_1)\n",
    "                loss = mse(mini_y_hat,mini_y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples_per_batch\n",
    "            losses.append(avg_epoch_loss)\n",
    "            # print(f'Epoch:{epoch}--> Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch >=epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "            \n",
    "    else:\n",
    "        while avg_epoch_loss >= epsilon:\n",
    "            batch = torch.randperm(num_samples)\n",
    "            epoch_loss = 0\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "                indice = batch[i:i+batch_size]\n",
    "                mini_x,mini_y = x[indice],y[indice]\n",
    "                mini_y_hat = reg(mini_x, theta_0,theta_1)\n",
    "                loss = mse(mini_y_hat,mini_y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "            avg_epoch_loss = epoch_loss/num_samples_per_batch\n",
    "            losses.append(avg_epoch_loss)\n",
    "            # print(f'Epoch:{epoch}--> Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "\n",
    "    print(f'Mini Batch Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses,theta_0.grad.item(),theta_1.grad.item(),t0_hist,t1_hist\n",
    "    \n",
    "def stochastic_grad(x, y, lr=0.01, epsilon=0.001, epochs= 10000):\n",
    "    num_samples = x.size(0)\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    avg_epoch_loss = float('inf')\n",
    "    epoch = 1\n",
    "    if epochs is not None:\n",
    "        while avg_epoch_loss>=epsilon:\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_samples):\n",
    "                y_hat = reg(x[i], theta_0,theta_1)\n",
    "                loss =mse(y_hat, y[i])\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples\n",
    "            losses.append(avg_epoch_loss)\n",
    "            # print(f'Epoch:{epoch}-->Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch>= epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    else:\n",
    "        while avg_epoch_loss>=epsilon:\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_samples):\n",
    "                y_hat = reg(x, theta_0,theta_1)\n",
    "                loss =mse(y_hat, y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples\n",
    "            # print(f'Epoch:{epoch}-->Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            losses.append(avg_epoch_loss)\n",
    "            epoch += 1\n",
    "            if epoch>500 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    \n",
    "    print(f'Stochastic Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses, theta_0.grad.item(), theta_1.grad.item(),t0_hist,t1_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Batch Gradient Descent---->No of Epochs Caliculated:2000 Final loss:0.5957541567401083\n",
      "Mini Batch Gradient Descent---->No of Epochs Caliculated:1500 Final loss:0.6029963959646826\n",
      "Stochastic Gradient Descent---->No of Epochs Caliculated:1100 Final loss:0.6110106790944151\n"
     ]
    }
   ],
   "source": [
    "losses_full, theta_0_full, theta_1_full, t0_hist_full, t1_hist_full = full_batch_grad(x1_torch, y_torch)\n",
    "losses_mini, theta_0_mini, theta_1_mini, t0_hist_mini, t1_hist_mini = mini_batch_grad(x1_torch,y_torch)\n",
    "losses_stoc, theta_0_stoc, theta_1_stoc, t0_hist_stoc, t1_hist_stoc = stochastic_grad(x1_torch,y_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
