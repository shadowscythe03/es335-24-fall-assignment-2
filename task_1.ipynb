{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "    \n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x,theta_0,theta_1):\n",
    "    return theta_1*x + theta_0\n",
    "def mse(y_hat,y):\n",
    "    return torch.mean((y_hat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n"
     ]
    }
   ],
   "source": [
    "x1_torch = torch.from_numpy(x1)\n",
    "y_torch = torch.from_numpy(y)\n",
    "theta_0 = torch.tensor(1.0,requires_grad=True)\n",
    "theta_1 = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "y_hat = reg(x1_torch,theta_0,theta_1)\n",
    "loss = mse(y_hat,y_torch)\n",
    "loss.backward()\n",
    "theta_0_true_grad = theta_0.grad.item()\n",
    "theta_1_true_grad = theta_1.grad.item()\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n",
      "The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset: -5.447053959220648 -1.194682613387704\n"
     ]
    }
   ],
   "source": [
    "t0_stochastic_gradients = []\n",
    "t1_stochastic_gradients = []\n",
    "for i in range(num_samples):\n",
    "    theta_0.grad = None\n",
    "    theta_1.grad = None\n",
    "    y_hat = reg(x1_torch[i],theta_0,theta_1)\n",
    "    loss = mse(y_hat,y_torch[i])\n",
    "    loss.backward()\n",
    "    t0_stochastic_gradients.append(theta_0.grad.item())\n",
    "    t1_stochastic_gradients.append(theta_1.grad.item())\n",
    "\n",
    "t0_avg_grad = sum(t0_stochastic_gradients) / num_samples\n",
    "t1_avg_grad = sum(t1_stochastic_gradients)/num_samples\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)\n",
    "print('The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset:',t0_avg_grad,t1_avg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between avg of stochastic and true grad is 4.9918890354661016e-08 1.527369031251169e-08\n"
     ]
    }
   ],
   "source": [
    "print('The difference between avg of stochastic and true grad is',abs(theta_0_true_grad-t0_avg_grad),abs(theta_1_true_grad-t1_avg_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_batch_gd(x,y,lr = 0.01,epsilon = 0.001):\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    while loss>=epsilon:\n",
    "        theta_0.grad.zero_()\n",
    "        theta_1.grad.zero_()\n",
    "        y_hat = reg(x,theta_0,theta_1)\n",
    "        loss = mse(y_hat,y)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            theta_0 = theta_0-lr*theta_0.grad\n",
    "            theta_1 = theta_1-lr*theta_1.grad\n",
    "\n",
    "        t0_hist.append(theta_0.grad.item())\n",
    "        t1_hist.append(theta_1.grad.item())\n",
    "\n",
    "    return losses, theta_0.grad.item(), theta_1.grad.item(),t0_hist,t1_hist\n",
    "def mini_batch_gd(x,y,lr = 0.01,batch_size = 2,epochs = 15,epsilon = 0.001):\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
