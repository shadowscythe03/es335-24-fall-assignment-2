{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "    \n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg(x,theta_0,theta_1):\n",
    "    return theta_1*x + theta_0\n",
    "def mse(y_hat,y):\n",
    "    return torch.mean((y_hat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n"
     ]
    }
   ],
   "source": [
    "x1_torch = torch.from_numpy(x1)\n",
    "y_torch = torch.from_numpy(y)\n",
    "theta_0 = torch.tensor(1.0,requires_grad=True)\n",
    "theta_1 = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "y_hat = reg(x1_torch,theta_0,theta_1)\n",
    "loss = mse(y_hat,y_torch)\n",
    "loss.backward()\n",
    "theta_0_true_grad = theta_0.grad.item()\n",
    "theta_1_true_grad = theta_1.grad.item()\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The True gradient values of theta_0, theta_1: -5.447053909301758 -1.1946825981140137\n",
      "The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset: -5.447053959220648 -1.194682613387704\n"
     ]
    }
   ],
   "source": [
    "t0_stochastic_gradients = []\n",
    "t1_stochastic_gradients = []\n",
    "for i in range(num_samples):\n",
    "    theta_0.grad = None\n",
    "    theta_1.grad = None\n",
    "    y_hat = reg(x1_torch[i],theta_0,theta_1)\n",
    "    loss = mse(y_hat,y_torch[i])\n",
    "    loss.backward()\n",
    "    t0_stochastic_gradients.append(theta_0.grad.item())\n",
    "    t1_stochastic_gradients.append(theta_1.grad.item())\n",
    "\n",
    "t0_avg_grad = sum(t0_stochastic_gradients)/num_samples\n",
    "t1_avg_grad = sum(t1_stochastic_gradients)/num_samples\n",
    "print('The True gradient values of theta_0, theta_1:',theta_0_true_grad,theta_1_true_grad)\n",
    "print('The average value of stochastic gradient of theta_0, theta_1 for all points in the dataset:',t0_avg_grad,t1_avg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference between avg of stochastic and true grad is 4.9918890354661016e-08 1.527369031251169e-08\n"
     ]
    }
   ],
   "source": [
    "print('The difference between avg of stochastic and true grad is',abs(theta_0_true_grad-t0_avg_grad),abs(theta_1_true_grad-t1_avg_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_batch_grad(x,y,lr = 0.01,epsilon = 0.001,epochs = 5000):\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    epoch = 1\n",
    "    loss = torch.tensor(float('inf'))\n",
    "    if epochs is not None:\n",
    "        while loss.item()>=epsilon:\n",
    "            y_hat = reg(x,theta_0,theta_1)\n",
    "            loss = mse(y_hat,y)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                theta_0.data = theta_0-lr*theta_0.grad\n",
    "                theta_1.data = theta_1-lr*theta_1.grad\n",
    "\n",
    "            t0_hist.append(theta_0.item())\n",
    "            t1_hist.append(theta_1.item())\n",
    "            # print(f'Epoch:{epoch}--> Loss:{loss.item()}')\n",
    "            theta_0.grad.zero_()\n",
    "            theta_1.grad.zero_()\n",
    "            epoch += 1\n",
    "            if epoch>=epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    else:\n",
    "        while loss.item()>=epsilon:\n",
    "            y_hat = reg(x,theta_0,theta_1)\n",
    "            loss = mse(y_hat,y)\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                theta_0.data = theta_0-lr*theta_0.grad\n",
    "                theta_1.data = theta_1-lr*theta_1.grad\n",
    "\n",
    "            t0_hist.append(theta_0.item())\n",
    "            t1_hist.append(theta_1.item())\n",
    "            # print(f'Epoch:{epoch}--> Loss:{loss.item()}')\n",
    "            theta_0.grad.zero_()\n",
    "            theta_1.grad.zero_()\n",
    "            epoch += 1\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "                \n",
    "    print(f'Full Batch Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses, theta_0.grad.item(), theta_1.grad.item(),t0_hist,t1_hist\n",
    "def mini_batch_grad(x,y,lr = 0.01,batch_size = 2,epsilon = 0.001, epochs = 1500):\n",
    "    num_samples = x.size(0)\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    avg_epoch_loss = float('inf')\n",
    "    num_samples_per_batch = num_samples//batch_size\n",
    "    epoch = 1\n",
    "    if epochs is not None:\n",
    "        while avg_epoch_loss >= epsilon:\n",
    "            batch = torch.randperm(num_samples)\n",
    "            epoch_loss = 0\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "                indice = batch[i:i+batch_size]\n",
    "                mini_x,mini_y = x[indice],y[indice]\n",
    "                mini_y_hat = reg(mini_x, theta_0,theta_1)\n",
    "                loss = mse(mini_y_hat,mini_y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples_per_batch\n",
    "            losses.append(avg_epoch_loss)\n",
    "            # print(f'Epoch:{epoch}--> Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch >=epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "            \n",
    "    else:\n",
    "        while avg_epoch_loss >= epsilon:\n",
    "            batch = torch.randperm(num_samples)\n",
    "            epoch_loss = 0\n",
    "            for i in range(0,num_samples, batch_size):\n",
    "                indice = batch[i:i+batch_size]\n",
    "                mini_x,mini_y = x[indice],y[indice]\n",
    "                mini_y_hat = reg(mini_x, theta_0,theta_1)\n",
    "                loss = mse(mini_y_hat,mini_y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "            avg_epoch_loss = epoch_loss/num_samples_per_batch\n",
    "            losses.append(avg_epoch_loss)\n",
    "            # print(f'Epoch:{epoch}--> Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "\n",
    "    print(f'Mini Batch Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses,theta_0.grad.item(),theta_1.grad.item(),t0_hist,t1_hist\n",
    "    \n",
    "def stochastic_grad(x, y, lr=0.01, epsilon=0.001, epochs= 10000):\n",
    "    num_samples = x.size(0)\n",
    "    theta_0 = torch.tensor(0.0,requires_grad=True)\n",
    "    theta_1 = torch.tensor(0.0,requires_grad=True)\n",
    "    t0_hist = [theta_0.item()]\n",
    "    t1_hist = [theta_1.item()]\n",
    "    losses = []\n",
    "    avg_epoch_loss = float('inf')\n",
    "    epoch = 1\n",
    "    if epochs is not None:\n",
    "        while avg_epoch_loss>=epsilon:\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_samples):\n",
    "                y_hat = reg(x[i], theta_0,theta_1)\n",
    "                loss =mse(y_hat, y[i])\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples\n",
    "            losses.append(avg_epoch_loss)\n",
    "            print(f'Epoch:{epoch}-->Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            epoch += 1\n",
    "            if epoch>= epochs:\n",
    "                break\n",
    "            if epoch>1000 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    else:\n",
    "        while avg_epoch_loss>=epsilon:\n",
    "            epoch_loss = 0\n",
    "            for i in range(num_samples):\n",
    "                y_hat = reg(x, theta_0,theta_1)\n",
    "                loss =mse(y_hat, y)\n",
    "                epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    theta_0.data -= lr*theta_0.grad\n",
    "                    theta_1.data -= lr*theta_1.grad\n",
    "                t0_hist.append(theta_0.item())\n",
    "                t1_hist.append(theta_1.item())\n",
    "                theta_0.grad.zero_()\n",
    "                theta_1.grad.zero_()\n",
    "            avg_epoch_loss = epoch_loss/num_samples\n",
    "            # print(f'Epoch:{epoch}-->Avg Epoch Loss:{avg_epoch_loss}')\n",
    "            losses.append(avg_epoch_loss)\n",
    "            epoch += 1\n",
    "            if epoch>500 and epoch%100 == 0:\n",
    "                if losses[-5:].count(losses[-1])== 5:\n",
    "                    break\n",
    "    \n",
    "    print(f'Stochastic Gradient Descent---->No of Epochs Caliculated:{epoch} Final loss:{losses[-1]}')\n",
    "    return losses, theta_0.grad.item(), theta_1.grad.item(),t0_hist,t1_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Batch Gradient Descent---->No of Epochs Caliculated:2000 Final loss:0.5957541567401083\n",
      "Mini Batch Gradient Descent---->No of Epochs Caliculated:1500 Final loss:0.603239492889911\n",
      "Epoch:1-->Avg Epoch Loss:9.665574760781134\n",
      "Epoch:2-->Avg Epoch Loss:3.359490477893722\n",
      "Epoch:3-->Avg Epoch Loss:1.7423228651652571\n",
      "Epoch:4-->Avg Epoch Loss:1.1924672300909003\n",
      "Epoch:5-->Avg Epoch Loss:0.9430930451065574\n",
      "Epoch:6-->Avg Epoch Loss:0.8081935674541008\n",
      "Epoch:7-->Avg Epoch Loss:0.7295188373196498\n",
      "Epoch:8-->Avg Epoch Loss:0.6824056090150223\n",
      "Epoch:9-->Avg Epoch Loss:0.6539661834577192\n",
      "Epoch:10-->Avg Epoch Loss:0.6367739230524919\n",
      "Epoch:11-->Avg Epoch Loss:0.6263919907100048\n",
      "Epoch:12-->Avg Epoch Loss:0.6201368882920935\n",
      "Epoch:13-->Avg Epoch Loss:0.6163802099113462\n",
      "Epoch:14-->Avg Epoch Loss:0.6141340064571434\n",
      "Epoch:15-->Avg Epoch Loss:0.6127984197550671\n",
      "Epoch:16-->Avg Epoch Loss:0.612010357741376\n",
      "Epoch:17-->Avg Epoch Loss:0.6115499643855358\n",
      "Epoch:18-->Avg Epoch Loss:0.6112850830826757\n",
      "Epoch:19-->Avg Epoch Loss:0.6111356002652721\n",
      "Epoch:20-->Avg Epoch Loss:0.611053961393594\n",
      "Epoch:21-->Avg Epoch Loss:0.6110112914440877\n",
      "Epoch:22-->Avg Epoch Loss:0.6109911318780237\n",
      "Epoch:23-->Avg Epoch Loss:0.6109832423154282\n",
      "Epoch:24-->Avg Epoch Loss:0.6109818602130277\n",
      "Epoch:25-->Avg Epoch Loss:0.6109837247609612\n",
      "Epoch:26-->Avg Epoch Loss:0.6109867661687637\n",
      "Epoch:27-->Avg Epoch Loss:0.610990350229592\n",
      "Epoch:28-->Avg Epoch Loss:0.610993860766526\n",
      "Epoch:29-->Avg Epoch Loss:0.6109968629067996\n",
      "Epoch:30-->Avg Epoch Loss:0.610999575043836\n",
      "Epoch:31-->Avg Epoch Loss:0.6110017655135281\n",
      "Epoch:32-->Avg Epoch Loss:0.6110035753833679\n",
      "Epoch:33-->Avg Epoch Loss:0.6110050840436245\n",
      "Epoch:34-->Avg Epoch Loss:0.6110062201339026\n",
      "Epoch:35-->Avg Epoch Loss:0.6110071214468774\n",
      "Epoch:36-->Avg Epoch Loss:0.6110079353116212\n",
      "Epoch:37-->Avg Epoch Loss:0.6110086210413342\n",
      "Epoch:38-->Avg Epoch Loss:0.6110089649675077\n",
      "Epoch:39-->Avg Epoch Loss:0.611009377825389\n",
      "Epoch:40-->Avg Epoch Loss:0.6110096431697546\n",
      "Epoch:41-->Avg Epoch Loss:0.6110098834250381\n",
      "Epoch:42-->Avg Epoch Loss:0.6110101090239065\n",
      "Epoch:43-->Avg Epoch Loss:0.6110102483853614\n",
      "Epoch:44-->Avg Epoch Loss:0.6110102951785217\n",
      "Epoch:45-->Avg Epoch Loss:0.6110104674604392\n",
      "Epoch:46-->Avg Epoch Loss:0.6110104502375836\n",
      "Epoch:47-->Avg Epoch Loss:0.6110104847709659\n",
      "Epoch:48-->Avg Epoch Loss:0.6110105198156147\n",
      "Epoch:49-->Avg Epoch Loss:0.6110106231731094\n",
      "Epoch:50-->Avg Epoch Loss:0.6110105528495369\n",
      "Epoch:51-->Avg Epoch Loss:0.6110105993958529\n",
      "Epoch:52-->Avg Epoch Loss:0.6110106513521852\n",
      "Epoch:53-->Avg Epoch Loss:0.6110106871107404\n",
      "Epoch:54-->Avg Epoch Loss:0.6110107178152002\n",
      "Epoch:55-->Avg Epoch Loss:0.6110106692834121\n",
      "Epoch:56-->Avg Epoch Loss:0.6110106995274797\n",
      "Epoch:57-->Avg Epoch Loss:0.6110106756334654\n",
      "Epoch:58-->Avg Epoch Loss:0.6110106693683444\n",
      "Epoch:59-->Avg Epoch Loss:0.6110106844510865\n",
      "Epoch:60-->Avg Epoch Loss:0.6110106844508255\n",
      "Epoch:61-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:62-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:63-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:64-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:65-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:66-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:67-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:68-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:69-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:70-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:71-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:72-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:73-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:74-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:75-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:76-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:77-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:78-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:79-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:80-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:81-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:82-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:83-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:84-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:85-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:86-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:87-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:88-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:89-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:90-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:91-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:92-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:93-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:94-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:95-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:96-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:97-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:98-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:99-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:100-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:101-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:102-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:103-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:104-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:105-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:106-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:107-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:108-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:109-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:110-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:111-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:112-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:113-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:114-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:115-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:116-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:117-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:118-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:119-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:120-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:121-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:122-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:123-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:124-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:125-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:126-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:127-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:128-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:129-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:130-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:131-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:132-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:133-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:134-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:135-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:136-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:137-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:138-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:139-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:140-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:141-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:142-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:143-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:144-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:145-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:146-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:147-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:148-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:149-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:150-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:151-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:152-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:153-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:154-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:155-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:156-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:157-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:158-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:159-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:160-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:161-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:162-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:163-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:164-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:165-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:166-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:167-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:168-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:169-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:170-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:171-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:172-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:173-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:174-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:175-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:176-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:177-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:178-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:179-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:180-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:181-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:182-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:183-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:184-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:185-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:186-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:187-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:188-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:189-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:190-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:191-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:192-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:193-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:194-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:195-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:196-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:197-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:198-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:199-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:200-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:201-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:202-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:203-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:204-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:205-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:206-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:207-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:208-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:209-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:210-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:211-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:212-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:213-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:214-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:215-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:216-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:217-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:218-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:219-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:220-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:221-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:222-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:223-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:224-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:225-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:226-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:227-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:228-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:229-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:230-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:231-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:232-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:233-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:234-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:235-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:236-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:237-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:238-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:239-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:240-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:241-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:242-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:243-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:244-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:245-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:246-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:247-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:248-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:249-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:250-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:251-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:252-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:253-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:254-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:255-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:256-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:257-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:258-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:259-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:260-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:261-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:262-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:263-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:264-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:265-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:266-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:267-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:268-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:269-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:270-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:271-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:272-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:273-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:274-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:275-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:276-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:277-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:278-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:279-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:280-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:281-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:282-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:283-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:284-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:285-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:286-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:287-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:288-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:289-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:290-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:291-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:292-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:293-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:294-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:295-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:296-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:297-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:298-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:299-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:300-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:301-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:302-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:303-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:304-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:305-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:306-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:307-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:308-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:309-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:310-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:311-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:312-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:313-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:314-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:315-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:316-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:317-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:318-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:319-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:320-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:321-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:322-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:323-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:324-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:325-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:326-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:327-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:328-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:329-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:330-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:331-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:332-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:333-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:334-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:335-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:336-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:337-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:338-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:339-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:340-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:341-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:342-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:343-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:344-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:345-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:346-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:347-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:348-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:349-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:350-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:351-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:352-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:353-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:354-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:355-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:356-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:357-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:358-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:359-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:360-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:361-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:362-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:363-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:364-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:365-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:366-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:367-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:368-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:369-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:370-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:371-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:372-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:373-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:374-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:375-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:376-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:377-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:378-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:379-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:380-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:381-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:382-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:383-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:384-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:385-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:386-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:387-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:388-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:389-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:390-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:391-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:392-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:393-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:394-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:395-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:396-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:397-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:398-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:399-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:400-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:401-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:402-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:403-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:404-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:405-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:406-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:407-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:408-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:409-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:410-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:411-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:412-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:413-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:414-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:415-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:416-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:417-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:418-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:419-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:420-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:421-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:422-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:423-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:424-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:425-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:426-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:427-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:428-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:429-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:430-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:431-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:432-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:433-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:434-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:435-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:436-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:437-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:438-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:439-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:440-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:441-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:442-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:443-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:444-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:445-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:446-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:447-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:448-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:449-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:450-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:451-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:452-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:453-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:454-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:455-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:456-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:457-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:458-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:459-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:460-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:461-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:462-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:463-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:464-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:465-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:466-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:467-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:468-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:469-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:470-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:471-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:472-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:473-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:474-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:475-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:476-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:477-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:478-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:479-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:480-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:481-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:482-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:483-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:484-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:485-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:486-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:487-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:488-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:489-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:490-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:491-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:492-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:493-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:494-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:495-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:496-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:497-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:498-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:499-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:500-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:501-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:502-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:503-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:504-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:505-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:506-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:507-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:508-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:509-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:510-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:511-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:512-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:513-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:514-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:515-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:516-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:517-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:518-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:519-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:520-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:521-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:522-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:523-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:524-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:525-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:526-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:527-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:528-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:529-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:530-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:531-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:532-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:533-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:534-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:535-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:536-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:537-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:538-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:539-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:540-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:541-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:542-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:543-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:544-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:545-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:546-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:547-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:548-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:549-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:550-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:551-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:552-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:553-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:554-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:555-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:556-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:557-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:558-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:559-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:560-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:561-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:562-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:563-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:564-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:565-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:566-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:567-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:568-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:569-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:570-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:571-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:572-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:573-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:574-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:575-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:576-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:577-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:578-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:579-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:580-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:581-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:582-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:583-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:584-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:585-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:586-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:587-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:588-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:589-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:590-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:591-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:592-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:593-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:594-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:595-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:596-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:597-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:598-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:599-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:600-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:601-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:602-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:603-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:604-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:605-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:606-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:607-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:608-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:609-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:610-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:611-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:612-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:613-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:614-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:615-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:616-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:617-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:618-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:619-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:620-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:621-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:622-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:623-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:624-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:625-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:626-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:627-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:628-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:629-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:630-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:631-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:632-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:633-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:634-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:635-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:636-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:637-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:638-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:639-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:640-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:641-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:642-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:643-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:644-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:645-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:646-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:647-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:648-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:649-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:650-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:651-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:652-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:653-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:654-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:655-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:656-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:657-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:658-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:659-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:660-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:661-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:662-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:663-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:664-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:665-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:666-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:667-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:668-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:669-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:670-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:671-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:672-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:673-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:674-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:675-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:676-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:677-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:678-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:679-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:680-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:681-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:682-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:683-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:684-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:685-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:686-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:687-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:688-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:689-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:690-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:691-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:692-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:693-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:694-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:695-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:696-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:697-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:698-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:699-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:700-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:701-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:702-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:703-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:704-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:705-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:706-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:707-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:708-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:709-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:710-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:711-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:712-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:713-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:714-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:715-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:716-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:717-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:718-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:719-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:720-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:721-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:722-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:723-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:724-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:725-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:726-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:727-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:728-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:729-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:730-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:731-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:732-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:733-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:734-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:735-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:736-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:737-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:738-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:739-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:740-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:741-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:742-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:743-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:744-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:745-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:746-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:747-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:748-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:749-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:750-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:751-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:752-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:753-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:754-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:755-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:756-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:757-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:758-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:759-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:760-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:761-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:762-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:763-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:764-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:765-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:766-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:767-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:768-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:769-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:770-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:771-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:772-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:773-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:774-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:775-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:776-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:777-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:778-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:779-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:780-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:781-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:782-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:783-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:784-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:785-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:786-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:787-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:788-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:789-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:790-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:791-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:792-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:793-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:794-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:795-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:796-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:797-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:798-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:799-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:800-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:801-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:802-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:803-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:804-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:805-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:806-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:807-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:808-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:809-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:810-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:811-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:812-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:813-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:814-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:815-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:816-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:817-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:818-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:819-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:820-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:821-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:822-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:823-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:824-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:825-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:826-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:827-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:828-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:829-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:830-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:831-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:832-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:833-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:834-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:835-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:836-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:837-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:838-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:839-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:840-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:841-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:842-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:843-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:844-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:845-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:846-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:847-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:848-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:849-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:850-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:851-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:852-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:853-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:854-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:855-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:856-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:857-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:858-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:859-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:860-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:861-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:862-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:863-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:864-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:865-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:866-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:867-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:868-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:869-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:870-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:871-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:872-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:873-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:874-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:875-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:876-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:877-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:878-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:879-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:880-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:881-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:882-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:883-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:884-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:885-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:886-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:887-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:888-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:889-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:890-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:891-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:892-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:893-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:894-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:895-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:896-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:897-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:898-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:899-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:900-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:901-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:902-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:903-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:904-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:905-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:906-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:907-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:908-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:909-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:910-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:911-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:912-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:913-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:914-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:915-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:916-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:917-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:918-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:919-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:920-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:921-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:922-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:923-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:924-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:925-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:926-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:927-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:928-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:929-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:930-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:931-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:932-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:933-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:934-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:935-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:936-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:937-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:938-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:939-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:940-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:941-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:942-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:943-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:944-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:945-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:946-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:947-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:948-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:949-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:950-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:951-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:952-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:953-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:954-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:955-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:956-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:957-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:958-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:959-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:960-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:961-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:962-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:963-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:964-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:965-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:966-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:967-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:968-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:969-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:970-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:971-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:972-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:973-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:974-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:975-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:976-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:977-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:978-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:979-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:980-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:981-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:982-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:983-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:984-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:985-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:986-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:987-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:988-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:989-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:990-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:991-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:992-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:993-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:994-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:995-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:996-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:997-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:998-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:999-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1000-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1001-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1002-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1003-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1004-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1005-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1006-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1007-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1008-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1009-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1010-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1011-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1012-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1013-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1014-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1015-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1016-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1017-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1018-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1019-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1020-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1021-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1022-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1023-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1024-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1025-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1026-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1027-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1028-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1029-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1030-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1031-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1032-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1033-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1034-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1035-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1036-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1037-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1038-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1039-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1040-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1041-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1042-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1043-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1044-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1045-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1046-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1047-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1048-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1049-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1050-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1051-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1052-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1053-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1054-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1055-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1056-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1057-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1058-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1059-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1060-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1061-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1062-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1063-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1064-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1065-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1066-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1067-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1068-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1069-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1070-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1071-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1072-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1073-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1074-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1075-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1076-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1077-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1078-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1079-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1080-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1081-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1082-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1083-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1084-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1085-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1086-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1087-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1088-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1089-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1090-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1091-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1092-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1093-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1094-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1095-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1096-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1097-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1098-->Avg Epoch Loss:0.6110106790944151\n",
      "Epoch:1099-->Avg Epoch Loss:0.6110106790944151\n",
      "Stochastic Gradient Descent---->No of Epochs Caliculated:1100 Final loss:0.6110106790944151\n"
     ]
    }
   ],
   "source": [
    "losses_full, theta_0_full, theta_1_full, t0_hist_full, t1_hist_full = full_batch_grad(x1_torch, y_torch)\n",
    "losses_mini, theta_0_mini, theta_1_mini, t0_hist_mini, t1_hist_mini = mini_batch_grad(x1_torch,y_torch)\n",
    "losses_stoc, theta_0_stoc, theta_1_stoc, t0_hist_stoc, t1_hist_stoc = stochastic_grad(x1_torch,y_torch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
